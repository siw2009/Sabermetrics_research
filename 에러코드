import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from io import StringIO

driver_path = "C:\\Users\\ziont\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe"
url = "https://www.mlb.com/stats/all-time-totals"

service = Service(driver_path)
driver = webdriver.Chrome(service=service)
wait = WebDriverWait(driver, 10)

driver.get(url)
#팝업
try:
    close_btn = wait.until(EC.element_to_be_clickable((By.XPATH, "//button[@aria-label='Close']")))
    close_btn.click()
    time.sleep(1)
except Exception:
    pass

def scrape_all_pages():
    data_list = []
    page_num = 1
    
    while True:
        for i in range(3):
            try:
                table = wait.until(EC.presence_of_element_located((By.CLASS_NAME, "bui-table")))
                html = table.get_attribute('outerHTML')
                df = pd.read_html(StringIO(html))[0]

                if not df.empty: 
                    break
                else:
                    print(f"1")
                    time.sleep(1)
            except Exception:
                print(f"2")
                time.sleep(1)

        data_list.append(df)
        print(f"[페이지 {page_num}] 행 개수: {len(df)}")

        
        try:
            next_btn = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@aria-label="next page button"]')))
            if 'disabled' in next_btn.get_attribute('class'):
                print("마지막 페이지")
                break

            
            wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@aria-label="next page button"]')))
            wait.until(EC.staleness_of(table)) 
            next_btn.click()
            page_num += 1

            
            wait.until(EC.presence_of_element_located((By.CLASS_NAME, "bui-table")))
        except Exception:
            print("끝")
            break

    return data_list

print("스크래핑 시작")
all_data = scrape_all_pages()

# ===== 저장 =====
if all_data:
    final_df = pd.concat(all_data, ignore_index=True)
    save_path = "C:\\Users\\ziont\\OneDrive\\문서\\R&E\\MLB_dataset.xlsx"
    final_df.to_excel(save_path, index=False)
    print(f"데이터 저장 완료: {save_path} / 총 {len(final_df)}행")
else:
    print(" 수집된 데이터가 없습니다.")

driver.quit()
